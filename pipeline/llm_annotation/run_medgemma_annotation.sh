#!/bin/bash

# ==============================================================================
# LLM-as-a-Judge 평가 자동화 스크립트
# ==============================================================================
#
# 이 스크립트는 여러 모델이 생성한 결과물들을 지정된 평가자 모델을 사용하여
# 순차적으로 평가합니다.
#
# 사용법:
# 1. 아래 'Configuration' 섹션의 변수들을 실험 환경에 맞게 수정합니다.
# 2. 터미널에서 ./run_all_evaluations.sh 를 실행합니다.
#
# ==============================================================================

# --- Configuration ---

# 1. 대기할 GPU ID 목록 (공백으로 구분)
#    이 GPU들에 실행 중인 프로세스가 없을 때까지 스크립트가 대기합니다.
WAIT_FOR_GPUS=(4 5 6)

# 2. 대기 시간 (초)
WAIT_INTERVAL=600

# --- GPU 대기 로직 ---
echo "======================================================"
echo ">> CHECKING FOR PROCESSES ON SPECIFIC GPUS: ${WAIT_FOR_GPUS[@]}"
echo "======================================================"

while true; do
    # 현재 실행 중인 모든 컴퓨팅 프로세스의 GPU ID 목록을 가져옴
    # nvidia-smi의 프로세스 섹션에서 숫자(GPU ID)만 추출
    running_gpu_ids=$(nvidia-smi | awk '
        # 프로세스 섹션 플래그 초기화
        BEGIN { in_process_section=0 }
        # "+--...+" 라인을 만나면 플래그를 끄고, "Processes:"를 만나면 켬
        /^\+--/ { in_process_section=0 }
        /Processes:/ { in_process_section=1; next }
        # 프로세스 섹션 안에서, 첫 번째 필드가 숫자인 라인만 처리
        in_process_section && /^[| ]*[0-9]/ { print $2 }
    ')

    # 대기해야 할 프로세스가 있는지 확인할 플래그
    still_busy=0

    # 대기 대상 GPU 목록을 순회
    for target_gpu in "${WAIT_FOR_GPUS[@]}"; do
        # 현재 실행 중인 GPU ID 목록에 대기 대상 GPU가 포함되어 있는지 확인
        if echo "${running_gpu_ids}" | grep -q -w "${target_gpu}"; then
            echo "[INFO] GPU ${target_gpu} is still busy."
            still_busy=1
        fi
    done

    # 만약 바쁜 GPU가 하나도 없다면 루프를 종료
    if [ ${still_busy} -eq 0 ]; then
        echo "\n[SUCCESS] All target GPUs (${WAIT_FOR_GPUS[@]}) are now clear."
        break
    fi

    # 바쁜 GPU가 있다면, 대기 메시지를 출력하고 잠시 대기
    echo "[$(date)] Waiting for target GPU processes to finish. Checking again in ${WAIT_INTERVAL} seconds..."
    sleep ${WAIT_INTERVAL}
done

echo "\n======================================================"
echo ">> Starting new experiments."
echo "======================================================"
echo "\n"

# 1. 평가 대상 모델 목록 (RAG 파이프라인을 실행한 모델들)
#    이 이름들은 입력 파일 경로의 일부로 사용됩니다.
TARGET_MODELS=(
    # "llama4"
    # "medgemma-27b"
    # "qwen3-32b"
    # "llama3.3-70b"
    # "qwen3-8b"
    "llama3.1-8b"
)

# 2. 데이터셋 목록 (각 모델이 결과를 생성한 데이터셋들)
#    이 이름들은 입력 및 출력 파일 경로의 일부로 사용됩니다.
DATASETS=(
    "chexpert"
    "mimic-iv"
    "rexgradient"
    "yale_internal"
    "mimic-cxr"
)

# 3. 평가자(Judge) 모델 설정 파일
#    어떤 모델을 "심판"으로 사용할지 결정합니다.
#    예: configs/medgemma.json, configs/gpt4.json 등
JUDGE_CONFIG_FILE="configs/medgemma.json"

# 4. 입력 및 출력 기본 경로
BASE_INPUT_DIR="../result"
BASE_OUTPUT_DIR="./result"

# --- Main Execution Logic ---

# 설정 파일에서 평가자 모델 이름을 추출하여 출력 디렉토리에 사용
JUDGE_NAME=$(basename ${JUDGE_CONFIG_FILE} .json) # 예: medgemma

echo "======================================================"
echo ">> JUDGE MODEL: ${JUDGE_NAME}"
echo ">> TARGET MODELS: ${TARGET_MODELS[@]}"
echo ">> DATASETS: ${DATASETS[@]}"
echo "======================================================"
echo ""

# 모든 대상 모델에 대해 반복
for model in "${TARGET_MODELS[@]}"; do
    echo "------------------------------------------------------"
    echo ">> Evaluating results generated by: ${model}"
    echo "------------------------------------------------------"

    # 모든 데이터셋에 대해 반복
    for dataset in "${DATASETS[@]}"; do
        
        # 데이터셋 이름에 따라 입력 파일 이름 패턴을 동적으로 설정
        input_filename=""
        case $dataset in
            "chexpert")
                input_filename="chexpert-plus_sampled2000_local_vllm_feedback.json"
                ;;
            "mimic-cxr")
                input_filename="mimic-cxr_sampled2000_local_vllm_feedback.json"
                ;;
            "mimic-iv")
                input_filename="mimic-iv-note_sampled2000_local_vllm_feedback.json"
                ;;
            "rexgradient")
                input_filename="ReXGradient-160K_sampled2000_local_vllm_feedback.json"
                ;;
            "yale_internal")
                input_filename="yale_internal_local_vllm_feedback.json"
                ;;
            *)
                echo "[WARN] Unknown dataset: $dataset. Skipping."
                continue
                ;;
        esac

        # 최종 입력 파일 및 출력 디렉토리 경로 구성
        INPUT_FILE="${BASE_INPUT_DIR}/${model}/${dataset}_2000/${input_filename}"
        if [ "$dataset" == "yale_internal" ]; then
            INPUT_FILE="${BASE_INPUT_DIR}/${model}/yale_internal/${input_filename}"
        fi
        
        OUTPUT_DIR="${BASE_OUTPUT_DIR}/${JUDGE_NAME}/${dataset}/${model}"

        # 입력 파일이 존재하는지 확인
        if [ ! -f "$INPUT_FILE" ]; then
            echo "[ERROR] Input file not found for ${model} on ${dataset}. Skipping."
            echo "        Expected at: ${INPUT_FILE}"
            echo ""
            continue
        fi

        echo ">> STARTING EVALUATION: ${model} on ${dataset}"
        echo "   - Input: ${INPUT_FILE}"
        echo "   - Output Dir: ${OUTPUT_DIR}"
        echo "   - Judge Config: ${JUDGE_CONFIG_FILE}"
        
        # 평가 스크립트 실행
        python run_annotation.py \
            --input_file "${INPUT_FILE}" \
            --output_dir "${OUTPUT_DIR}" \
            --config_file "${JUDGE_CONFIG_FILE}"

        if [ $? -ne 0 ]; then
            echo "[FATAL] Evaluation failed for ${model} on ${dataset}. Exiting script."
            exit 1
        fi
        
        echo ">> COMPLETED: ${model} on ${dataset}"
        echo ""

    done
done

echo "======================================================"
echo ">> ALL EVALUATIONS COMPLETE"
echo "======================================================"